# La fragilidad estructural de la seguridad en LLMs: cómo la ingeniería de activaciones expone vulnerabilidades fundamentales

Las técnicas de Activation Engineering y CAA (Contrastive Activation Addition) han revelado que **los mecanismos de seguridad en modelos de lenguaje grandes ocupan apenas el 3% de los parámetros del modelo y pueden desactivarse completamente mediante intervenciones lineales simples**. Esta fragilidad fundamental no es un error de implementación sino una consecuencia estructural de cómo funcionan los métodos actuales de alineación como RLHF. Los mismos métodos que permiten mejoras del 20-30% en métricas de seguridad también logran tasas de éxito de jailbreak del 99% cuando se weaponizan, creando un dilema dual-use sin precedentes en IA.

La investigación reciente (2023-2025) demuestra que la seguridad en LLMs es mediada por direcciones unidimensionales en el espacio de activaciones, haciéndola trivialmente manipulable. Modificar rasgos de personalidad como la Concienciosidad produce **caídas de 20-40 puntos porcentuales en benchmarks de seguridad** mientras preserva capacidades generales, evidenciando que seguridad y capacidad son ejes ortogonales y que la primera es estructuralmente más frágil. Este descubrimiento desafía la viabilidad de los métodos actuales de alineación y ha generado debates intensos sobre si la transparencia fortalece o debilita la seguridad, cómo regular el acceso a estas técnicas, y si es posible lograr alineación robusta con las arquitecturas actuales de transformers.

## Vulnerabilidades técnicas documentadas: jailbreaks quirúrgicos a menos de $5

La investigación académica reciente ha expuesto múltiples vectores de ataque basados en manipulación de activaciones que operan a una fracción del costo de métodos tradicionales. El método ORTHO (Arditi et al., NeurIPS 2024) representa el caso más alarmante: mediante la ortogonalización de pesos contra una "dirección de rechazo" única, se puede jailbreakear completamente un modelo de 70B parámetros con **menos de $5 en cómputo y en aproximadamente 1 hora**, sin necesidad de optimización por gradientes ni ejemplos de completaciones dañinas.

**El método ORTHO funciona extrayendo la dirección de rechazo** mediante diferencias de medias entre activaciones de instrucciones dañinas y benignas, luego aplicando una modificación de rango uno a las matrices de pesos: W'_out ← W_out - r̂r̂ᵀW_out. Esto previene que el modelo escriba la dirección de rechazo al flujo residual. Los resultados son devastadores: en 13 modelos probados (1.8B-72B parámetros), las tasas de éxito de ataque (ASR) alcanzaron 79.9% en Llama-2 7B, 84.3% en Qwen 14B, y 78.0% en Qwen 72B en el benchmark HarmBench, mientras la degradación en MMLU fue menor al 1%. La única métrica que mostró caídas consistentes fue TruthfulQA (2-5 puntos), precisamente porque este benchmark incluye tópicos relacionados con rechazo.

La técnica de "Safety Patterns" (Li et al., 2024) logró resultados incluso más impresionantes. Mediante localización de índices de características con varianza mínima y debilitamiento de patrones de seguridad, alcanzaron **ASR del 95.4% en Llama2-7B, 94.8% en Llama2-13B, y 93.2% en Yi-34B** en el dataset AdvBench, partiendo de tasas originales cercanas a 0%. El análisis t-SNE reveló que las distribuciones de activación de consultas maliciosas se desplazan hacia las de consultas benignas cuando se debilitan los patrones de seguridad, proporcionando una base mecanística para entender por qué funcionan estos jailbreaks.

**La asimetría de costos computacionales es dramática**. Mientras ORTHO requiere menos de $5 para un modelo de 70B, el fine-tuning adversarial tradicional con LoRA cuesta $7-50 para un modelo de 7B, y el fine-tuning completo puede costar $100-1000+. CAA opera con overhead de inferencia insignificante (1-5% de latencia adicional) y solo requiere 50-2000 pares contrastivos en lugar de los 10,000+ ejemplos que necesita el fine-tuning. Crucialmente, los métodos de activación no requieren completaciones dañinas, solo prompts, reduciendo significativamente la barrera para ejecutar ataques.

La detectabilidad de estos ataques es alarmantemente baja. CAA opera puramente en tiempo de inferencia sin modificaciones persistentes en los pesos, haciendo el checksum del modelo idéntico al original. ORTHO crea pesos matemáticamente válidos que mantienen coherencia en inputs benignos y pasan validaciones estándar. La perplejidad permanece normal, los benchmarks de capacidad muestran degradación menor al 3%, y no hay firmas conductuales obvias. En contraste, los jailbreaks basados en prompts como GCG tienen alta detectabilidad (perplejidad elevada de los sufijos), y los ataques de fine-tuning pueden detectarse mediante comparación de pesos. Las únicas estrategias de detección viables para ataques de activación requieren descomposición de valores singulares (SVD) para detectar modificaciones de rango uno (ORTHO) o monitoreo costoso de activaciones en tiempo de ejecución (CAA), además de baterías extensivas de pruebas de seguridad.

**Más de 2000 modelos "abliterados"** (con el rechazo removido via variantes de ortogonalización) están disponibles en HuggingFace, demostrando la adopción generalizada de estas técnicas. Los repositorios de código (nrimsky/CAA, andyrdt/refusal_direction) están públicamente disponibles y ampliamente utilizados, con implementaciones listas para producción que cualquiera puede descargar y ejecutar.

## La hipótesis de representación lineal y el colapso de RLHF

La fragilidad observada en los mecanismos de seguridad tiene una base teórica profunda en la Hipótesis de Representación Lineal (LRH), que postula que **conceptos de alto nivel son representados como direcciones lineales en el espacio de representaciones**. Park et al. (2023) formalizaron esta conexión mostrando que si los conceptos son lineales, entonces la probabilidad de salida es logit-lineal respecto al concepto, permitiendo tanto medición (linear probing) como control (model steering) mediante operaciones vectoriales simples.

La investigación empírica valida extensivamente la LRH. Gurnee & Tegmark (2023) demostraron que espacio y tiempo se codifican linealmente en LLaMA-13B. Marks & Tegmark (2023) mostraron que direcciones de "verdad" pueden inducir salidas verdaderas o falsas. Los fenómenos de honestidad, power-seeking, y sycophancy todos exhiben representaciones lineales. Esta estructura no es accidental: el marco de Spectral Principal Path (SPP) de Tian et al. (2025) explica que las representaciones se propagan a través de un número pequeño de caminos espectrales principales, direcciones alineadas con valores singulares grandes en cada capa, lo que explica por qué **las direcciones de conceptos permanecen estables y linealmente accesibles a través de capas**.

**Esta misma estructura que permite interpretabilidad también garantiza vulnerabilidad**. Si los conceptos de seguridad siguen representación lineal, entonces: (1) son computacionalmente triviales de identificar mediante diferencias de medias sobre ejemplos contrastivos, (2) pueden manipularse mediante simple adición/sustracción vectorial sin optimización por gradientes, (3) ocupan subespacios de baja dimensión que requieren intervención mínima para modificar, y (4) múltiples conceptos pueden combinarse linealmente para ataques composicionales.

La investigación sobre poda y modificaciones de bajo rango (ICML 2024) cuantificó esta fragilidad: las regiones críticas para seguridad son notablemente escasas, **comprendiendo aproximadamente 3% a nivel de parámetros y 2.5% a nivel de rango**. Remover estas regiones compromete la seguridad mientras solo impacta levemente la utilidad. Arditi et al. demostraron que el comportamiento de rechazo es mediado por una dirección única en el flujo residual, y que prevenir que el modelo represente esta dirección inhibe su capacidad de rechazar solicitudes, mientras que añadirla artificialmente causa rechazo de solicitudes inofensivas.

**RLHF crea esta fragilidad mediante supervisión a nivel de salida** en lugar de integrar la seguridad en las representaciones profundas del modelo. Como Zou et al. (2024) articulan: "RLHF y adversarial training ofrecen supervisión a nivel de salida que induce estados de rechazo dentro del espacio de representación del modelo. Sin embargo, los estados dañinos permanecen accesibles una vez que estos estados iniciales de rechazo son bypasseados." El paper Circuit Breakers documenta que RLHF y DPO "frecuentemente fallan contra ataques adversariales de última generación" porque la seguridad no está integrada en el modelo del mundo del modelo sino aplicada como una capa superficial.

La separabilidad lineal entre seguridad y capacidades hace que la primera sea trivial de ablar. Los investigadores en Alignment Forum debaten si "RLHF es lo peor posible" frente al problema de alineación, con tres problemas fundamentales: (1) avance de capacidades que acorta el tiempo hasta conflictos de IA sin restricciones, (2) falsa confianza de que podría resolver alineación, y (3) encubrir desalineación haciendo que los modelos parezcan alineados en distribución pero fallen catastróficamente fuera de ella.

## Manipulación de personalidad: el vector de ataque previamente desconocido

Una revelación particularmente alarmante es que **la manipulación de rasgos de personalidad puede degradar sistemáticamente las salvaguardas de seguridad mientras preserva capacidades generales**, demostrando que seguridad y capacidad son ejes ortogonales contrario a las afirmaciones de "safetywashing" (Ren et al., 2024). Serapio-García et al. (2024) documentaron con precisión cuantitativa cómo configuraciones específicas de personalidad impactan la seguridad.

**Reducir Concienciosidad es catastrófico**: en GPT-4, produce caídas de 26.4 puntos porcentuales en ETHICS-Deontology y 26.4pp en ETHICS-Commonsense. En Llama-3-70B, las caídas son 22.0pp en ETHICS-Commonsense. Los tres modelos evaluados perdieron 20-40 puntos porcentuales en tareas de seguridad al reducir Concienciosidad. WMDP (conocimiento peligroso) y TruthfulQA también mostraron caídas sustanciales, mientras que MMLU (capacidad general) también disminuyó, indicando que este rasgo afecta tanto seguridad como capacidad.

**Elevar Extraversión degrada honestidad**: GPT-4 pierde 10 puntos en TruthfulQA, Llama-4 pierde 8 puntos, y Llama-3 pierde 6 puntos. El mecanismo propuesto es que Extraversión está asociada con manejo de impresiones y mayor disposición a emplear engaño para ganancia social, manifestándose como comportamiento tipo sycophancy que ofrece una explicación mecanística para este déficit de honestidad.

**Elevar Neuroticismo reduce ética**: En Llama-3-70B, produce una caída de 10pp en ETHICS-CM. Las combinaciones adversariales (Dark Triad: baja Agreeableness + baja Concienciosidad + alto Neuroticismo) generan caídas aún más dramáticas: GPT-4 pierde 26.4pp en ETHICS-CM, Llama-3-70B pierde 22.0pp, mientras las pérdidas en MMLU permanecen por debajo de 3pp, demostrando que "la personalidad adversarial degrada agudamente el comportamiento de seguridad a través de todas las familias de modelos mientras deja las capacidades generales casi sin cambios".

La investigación sobre estatus socioeconómico (2024) encontró que steering hacia "Poor Socioeconomic Status" produce **hasta 6.8 puntos de disminución** en puntajes de seguridad en detección de contenido ofensivo. El mecanismo propuesto es que características de background que reflejan inteligencia emocional inestable pueden interrumpir la capacidad del modelo de discernir matices sutiles en lenguaje y señales sociales. Perspectivas liberales conducen a una "postura más permisiva en ciertos tipos de expresión, inadvertidamente bajando el umbral de lo que el modelo considera ofensivo".

**El descubrimiento crítico es la decoupling de capacidad y seguridad**: "Aparte de la intervención extrema de Baja Concienciosidad, los cambios en métricas de seguridad son ampliamente independientes de cambios en capacidad". Por ejemplo, Agreeableness media incrementa TruthfulQA de Llama-4 sin efectos correspondientes en capacidad. Esto "desafía las afirmaciones de safetywashing" y demuestra que la personalidad define un eje ortogonal a la escala del modelo.

Los investigadores concluyen que "las ramificaciones son graves y de gran alcance, ya que la validez de los benchmarks de seguridad potencialmente no está dada si la evaluación de LLM no se modula con una amplia gama de personalidades sintetizadas". Llaman a "investigación urgente e inmediata sobre este nuevo vector de ataque previamente desconocido", ya que actores maliciosos podrían "explotar prompt engineering psicométrico para elicitar configuraciones de personalidad dañinas".

## El dilema dual-use: misma técnica, efectos opuestos

Las técnicas de ingeniería de activaciones presentan un **dilema dual-use sin precedentes en IA**: los mismos métodos permiten mejoras sustanciales de seguridad Y ataques devastadores. Rimsky et al. (2024) en su paper de CAA declaran explícitamente: "Nuestro método se alinea con el objetivo de hacer sistemas de IA más útiles, honestos e inofensivos. Al permitir steering preciso de salidas de modelos de lenguaje, CAA contribuye a reducir riesgos asociados con comportamientos desalineados o inseguros." Sin embargo, el mismo paper demuestra que CAA puede dirigir modelos tanto en direcciones positivas como negativas usando multiplicadores, creando una capacidad dual-use inherente.

**Las aplicaciones legítimas son sustanciales y cuantificables**. Representation Engineering (Zou et al., 2023) logró mejoras de 20-30 puntos porcentuales en TruthfulQA mediante vectores de "honestidad". CAA redujo sycophancy en 30-40 puntos porcentuales en Llama-2-7B, superando few-shot prompting. Adaptive Activation Steering reduce alucinaciones en tiempo real sin modificar el modelo. La mitigación de sesgo de género fue exitosa, con detección y supresión controlable de estereotipos en recomendaciones de carrera.

**La eficiencia es dramática comparada con RLHF**: CAA es efectivo con 100-500 pares contrastivos mientras RLHF requiere 10,000+ anotaciones de preferencia humana, reduciendo el costo de etiquetado en ~95%. El costo computacional es esencialmente cero (sin entrenamiento, solo almacenamiento vectorial que es ~0.01% del tamaño del modelo), con overhead de inferencia menor al 5%. La degradación de MMLU es menor al 6% para la mayoría de vectores de steering, y algunos vectores (truthfulness) en realidad mejoran el rendimiento de MMLU.

**La personalización tiene valor económico masivo**: StyleVector (Zhang et al., 2025) logra mejora relativa del 8% en calidad de personalización con reducción de almacenamiento de 1700x versus fine-tuning. McKinsey (2024) estima que compañías que logran personalización en el cuartil superior generan 40% más ingresos de esas actividades, con valor potencial a través de industrias en EE.UU. superior a $1 trillón. Más del 70% de consumidores esperan personalización como línea base.

Sin embargo, **los riesgos maliciosos están igualmente bien documentados**. El Trojan Activation Attack (TA²) de Wang & Shu (2023) logra tasas de éxito cercanas al 100% en modelos alineados sin defensa, inyectando vectores de steering maliciosos en tiempo de inferencia sin modificar pesos del modelo. La técnica es altamente sigilosa, opera sin envenenamiento de datos de entrenamiento ni manipulación obvia de prompts, y añade poco o ningún overhead comparado con inferencia estándar.

Los vectores de ataque en sistemas de producción son múltiples. Los modelos de pesos abiertos enfrentan riesgo **4-6x mayor** que modelos de solo-API debido a acceso white-box que permite optimización local de ataques. La distribución irreversible significa que una vez liberados, no pueden ser revocados. Cisco AI Defense (2024) encontró que ataques multi-turno logran tasas de éxito superiores al 60% versus 10-15% para ataques de turno único, particularmente efectivos contra modelos de pesos abiertos con conversaciones extendidas.

El caso real más alarmante es la revelación de Anthropic en noviembre 2024 de actores patrocinados por el estado chino usando técnicas de "manipulación" (jailbreaking) con Claude para conducir operaciones cibernéticas contra ~30 organizaciones, demostrando que las implicaciones de seguridad de la manipulación de activaciones no son teóricas sino operacionales.

## Marcos de divulgación y las posiciones de los laboratorios principales

El debate sobre "security through obscurity" versus divulgación abierta es central en la comunidad de IA, con implicaciones profundas para cómo se manejan las vulnerabilidades de activación. Hall et al. (2025) proporcionan el análisis más comprehensivo, argumentando que **décadas de evidencia en ciberseguridad demuestran que la transparencia promueve mejores prácticas de seguridad**.

**Los argumentos contra security by obscurity son históricos y empíricos**. El Principio de Kerckhoffs (1883) estableció que "el sistema no debe requerir secreto y puede ser robado por el enemigo sin causar problemas". La Máxima de Shannon refuerza: "el enemigo conoce el sistema siendo usado". La Ley de Linus ("dados suficientes ojos, todos los bugs son superficiales") ha sido validada repetidamente. Los problemas clave con obscuridad incluyen: falso sentido de seguridad, los secretos eventualmente se filtran mediante ingeniería inversa o amenazas internas, impide investigación que encontraría vulnerabilidades, retrasa mitigaciones, y reduce confianza al prevenir verificación independiente.

Sin embargo, **la IA presenta desafíos únicos** que complican la aplicación directa de principios de ciberseguridad. El entrelazamiento de datos de entrenamiento significa que la funcionalidad de IA es inseparable de los datos, haciendo que la divulgación arriesgue exponer datos privados/propietarios. El gaming de métricas (Ley de Goodhart) significa que divulgar funciones objetivo puede facilitar su manipulación. La fragilidad de los sistemas de IA crea trade-offs entre privacidad, rendimiento, seguridad y alineación donde cambios para mitigar una vulnerabilidad pueden introducir otras. La velocidad de desarrollo es más rápida que la experimentada históricamente por la comunidad de seguridad, dejando menos tiempo para pruebas y escrutinio comunitario antes del despliegue.

**El consenso emergente favorece un enfoque moderado**: transparencia completa de arquitectura y capacidades del modelo, divulgación selectiva de vulnerabilidades específicas con coordinación, procesos fuertes de bug bounty y divulgación responsable, documentación pública de modelos de amenaza y limitaciones, y protección para investigadores de seguridad de buena fe. Como Hall et al. articulan: "Security by transparency es un paradigma, no un conjunto rígidamente definido de reglas. En una situación dada, proporciona principios con los cuales razonar sobre qué divulgar a quién, qué mantener secreto, y las posibles consecuencias."

**Anthropic ha implementado el marco más comprehensivo** con su Responsible Scaling Policy (RSP) v2.2, que establece AI Safety Levels graduados (ASL-1 a ASL-4+) con salvaguardas más fuertes para capacidades superiores. ASL-3 requiere protección contra insiders sofisticados y actores comprometidos por estados. Los despliegues multi-capa incluyen clasificadores de prompt/completion en tiempo real, sistemas de monitoreo asíncrono, detección post-hoc de jailbreaks y procedimientos de respuesta rápida. Su política de divulgación (actualizada febrero 2025) establece: "Hemos establecido este programa de divulgación responsable para colaborar con investigadores de seguridad que ayudan a identificar vulnerabilidades potenciales en nuestros sistemas", con provisiones de safe harbor explícitas.

**OpenAI estableció en 2025 la primera política formal de divulgación saliente** (Outbound Coordinated Disclosure) para vulnerabilidades que ELLOS descubren en software de terceros, anticipando que "sistemas desarrollados por OpenAI ya han descubierto vulnerabilidades zero-day en software de terceros y código abierto". La política es intencionalmente flexible con "timelines abiertos por defecto" ya que "sistemas de IA se vuelven más efectivos razonando sobre código... pueden requerir colaboración más profunda y más tiempo para resolver sosteniblemente". Esto ha generado críticas de investigadores sobre non-disclosure clauses en términos de bug bounty y preocupaciones sobre timelines abiertos dejando usuarios vulnerables.

**El Institute for AI Policy and Strategy (IAPS) propone un marco de Divulgación Coordinada de Capacidades Dual-Use (CDDC)** con componentes clave: un clearinghouse de información donde un "coordinador" dentro del gobierno de EE.UU. recibe reportes de capacidades dual-use (DUC), vías de reporte obligatorio/voluntario, grupos de trabajo de defensores donde agencias reciben reportes y desarrollan respuestas, y un enfoque multi-stakeholder involucrando desarrolladores de IA, agencias gubernamentales y actores del sector privado.

**NIST AI 800-1 (segundo borrador público, enero 2025)** establece prácticas específicas para modelos fundacionales dual-use, incluyendo la Práctica 6.4 (proveer safe harbors para investigación de seguridad de terceros) y Práctica 6.5 (crear bounties para problemas relacionados con riesgo de mal uso). El EU AI Act (efectivo agosto 2024, aplicación completa agosto 2026) requiere transparencia para sistemas de IA de alto riesgo, con obligaciones específicas de documentación técnica, métricas de rendimiento, riesgos conocidos y medidas de mitigación.

La diferencia entre **modelos de pesos abiertos versus solo-API** es fundamental para el perfil de riesgo. Los modelos de pesos abiertos permiten optimización white-box de ataques, no tienen control central de despliegue (no se pueden parchear, monitorear o revocar acceso post-liberación), y muestran tasas de éxito de ataque 4-6x mayores en ataques basados en activación. El reporte NTIA (2024) encuentra "elevación marginal de riesgo" para aplicaciones de ciberseguridad/ingeniería social. Modelos de solo-API permiten parcheo centralizado, capacidades de monitoreo, rate limiting, y protección black-box contra ataques basados en gradientes, pero tienen limitaciones de privacidad, dependencia de disponibilidad, y acumulación de costos.

## Defensas efectivas y recomendaciones para despliegue robusto

A pesar de la fragilidad estructural identificada, **existen defensas efectivas aunque costosas**. RA-LLM (Cao et al., 2024) representa el enfoque más robusto documentado, reduciendo la tasa de éxito de ataque del 99% al 10% o menos contra ataques adversariales GCG Y jailbreaks artesanales. La técnica implementa verificación robusta mediante muestreo repetido con perturbaciones, proporcionando garantías teóricas contra manipulaciones adversariales acotadas. Crucialmente, no requiere reentrenamiento del modelo, construyendo una función de verificación de alineación robusta sobre el modelo existente.

**El costo es moderado**: RA-LLM añade 20-30% de latencia de inferencia, que aunque significativo es manejable para muchos despliegues. SPO-VLM (Sequence-Level Preference Optimization) combina steering de activación con optimización RLHF en un proceso de dos etapas: (1) computar vectores de steering adaptivos para suprimir direcciones dañinas, y (2) refinar via optimización de preferencia con recompensas multi-objetivo. Supera defensas previas como ASTRA en múltiples datasets de jailbreak mientras mantiene rendimiento en tareas benignas superior al 90%.

Sin embargo, **la mayoría de defensas requieren acceso white-box**, limitando su aplicabilidad a despliegues de solo-API. Los filtros de perplejidad tienen efectividad variable (0-70% de reducción de ASR) y son efectivos principalmente contra GCG pero fallan contra jailbreaks escritos por humanos. SmoothLLM reduce ASR del 70% al 25% pero con degradación de precisión benigna al 80%. Los verificadores de integridad de modelo son preventivos pero computacionalmente costosos para modelos grandes.

**Las estrategias de detección son desafiantes pero viables**. Para ORTHO, la Descomposición de Valores Singulares (SVD) puede detectar modificaciones de rango uno, con verificación cruzada contra direcciones de rechazo conocidas y baterías extensivas de benchmarks de seguridad. Para CAA, se requiere monitoreo de activación en tiempo de ejecución (costoso), detección de cambios inesperados en patrones de activación de capas 15-17, y detección de inconsistencia conductual. El desafío fundamental es que ambos métodos producen modelos que funcionan bien en benchmarks estándar mientras fallan en pruebas específicas de seguridad.

**Las medidas defensivas propuestas incluyen enfoque defense-in-depth**: múltiples métodos imperfectos combinados, incluyendo RLHF + circuit breakers + filtros de input/output. Monitoreo de personalidad con "pipelines de despliegue que incluyan detección en línea de indicadores de persona". Evaluación continua contra manipulación de personalidad, steering de activación, y ataques de fine-tuning. Transparencia mediante "comprensión y validación de los procesos internos dentro de la IA".

**El marco de acceso escalonado (NTIA, Stanford CRFM)** propone tres niveles: (1) Pesos completamente abiertos para modelos bajo umbrales de capacidad con análisis de riesgo marginal antes de liberación, (2) Acceso estructurado para modelos de riesgo medio con acceso de investigador verificado, monitoreo de uso y rate limiting, sin optimización local permitida, y (3) Solo-API para modelos frontera de alto riesgo sin acceso a pesos, filtrado fuerte de salidas, y logging comprehensivo.

**Para implementación práctica, las recomendaciones específicas son**:

**Investigadores**: Continuar publicación abierta de técnicas centrales, implementar divulgación responsable para vulnerabilidades críticas, desarrollar y compartir herramientas defensivas junto con investigación ofensiva, cuantificar riesgo marginal para cada nueva capacidad.

**Desarrolladores de modelos**: Desplegar RA-LLM o defensas equivalentes en sistemas de producción, implementar verificación de integridad de modelo para liberaciones de pesos abiertos, conducir red-teaming basado en activación pre-liberación, monitorear patrones de steering anómalos post-despliegue.

**Formuladores de política**: Adoptar marcos de acceso escalonado basados en umbrales de capacidad, apoyar desarrollo de registros de modelo y tracking de proveniencia, financiar investigación defensiva a la par con investigación de capacidad, preparar protocolos de respuesta rápida para vectores de ataque recién descubiertos.

**Desplegadores**: Usar modelos de solo-API para aplicaciones de alto riesgo, implementar defense-in-depth (filtrado + monitoreo + RA-LLM), validar vectores de personalización antes del despliegue, monitorear patrones de ataque multi-turno.

## Conclusión: la urgencia de repensar la alineación fundamental

La convergencia de evidencia de ingeniería de activación, CAA, estudios de manipulación de personalidad, y trabajo teórico sobre representaciones lineales revela que **los métodos actuales de alineación enfrentan vulnerabilidades estructurales fundamentales que no son defectos menores de implementación sino consecuencias de la arquitectura básica de cómo se implementa la seguridad en LLMs modernos**. Los mecanismos de seguridad ocupan subespacios lineales escasos y de baja dimensión (~3% de parámetros), intervenciones simples (sustracción de vector único, prompting de personalidad, fine-tuning mínimo) bypasean completamente las salvaguardas, y la Hipótesis de Representación Lineal explica por qué: si los conceptos de seguridad son lineales, son trivialmente manipulables.

Las implicaciones son profundas para el futuro de la seguridad de IA. RLHF y métodos similares proporcionan solo seguridad superficial a nivel de salida que no se integra en las capacidades centrales del modelo. Los trade-offs personalidad-seguridad documentados con datos cuantitativos precisos (caídas de 20-40pp) demuestran que la seguridad no está robustamente integrada sino que existe como una overlay frágil que puede deshabilitarse a través de múltiples vectores de ataque. La comunidad de seguridad de IA está activamente debatiendo si mejoras incrementales a métodos actuales son suficientes, o si se necesitan enfoques fundamentalmente diferentes como circuit breakers, interpretabilidad mecanística, o incluso arquitecturas completamente nuevas.

**Lo más preocupante es el problema de escala temporal**: estas vulnerabilidades están siendo documentadas y explotadas mientras los modelos se vuelven más capaces y ampliamente desplegados. El balance actual de riesgo-beneficio favorece continuar investigación abierta (los beneficios cuantificados incluyen mejoras de 20-30% en métricas de seguridad, reducción del 95% en costo de alineación versus RLHF, y potencial de valor de $1T+ en personalización), pero este balance puede cambiar dramáticamente a medida que las capacidades se acercan a umbrales críticos en diseño biológico, ofensa cibernética autónoma, persuasión a escala, o auto-mejora recursiva.

La pregunta fundamental permanece sin respuesta: **¿es posible la alineación robusta con arquitecturas de transformers que inherentemente usan representaciones lineales, o necesitamos enfoques fundamentalmente diferentes?** La investigación actual sugiere que la seguridad debe moverse más allá de mecanismos de rechazo superficiales hacia integración profunda en los modelos del mundo de los modelos, representaciones no-lineales o distribuidas redundantemente, verificación formal de propiedades de seguridad, y arquitecturas diseñadas desde cero con robustez de alineación como principio central en lugar de un parche posterior. El tiempo para desarrollar estas soluciones se está agotando a medida que las capacidades de IA avanzan hacia territorios donde las fallas de alineación podrían tener consecuencias catastróficas e irreversibles.